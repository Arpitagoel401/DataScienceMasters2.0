{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c1fa74-5bfe-4c0f-bbfc-1d6aaf1189d9",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513168b7-fd62-4afb-b9e0-d27a34856a13",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks.\n",
    "It builds an ensemble of weak regression models (typically decision trees) sequentially, where each model corrects the errors of its predecessor.\n",
    "The model is trained by minimizing a loss function, often using gradient descent to update the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae52ffa-fdf5-4051-8083-7ffa9f59abad",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02214cb-4a37-42da-903e-694cbb2af14c",
   "metadata": {},
   "source": [
    "implementing a simple gradient boosting algorithm involves creating a class that iteratively fits weak learners (e.g., decision trees) to the residuals of the previous predictions and updates the predictions accordingly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64703080-c597-4ef3-b5ed-8e4289258153",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd266fd-9f1c-42c2-8a58-1effb793b0cb",
   "metadata": {},
   "source": [
    "To optimize the model, you can perform grid search or random search over a range of hyperparameters such as learning rate, number of trees, and tree depth.\n",
    "You would evaluate the performance of each combination of hyperparameters using cross-validation and select the combination that gives the best performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a558deb-35aa-47c4-8141-9687b530745e",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903d7c8-29b2-436c-80e2-061b3a7f2ea1",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing.\n",
    "Typically, weak learners are simple models, such as decision trees with shallow depth or linear models.\n",
    "In the context of Gradient Boosting, weak learners are sequentially combined to form a strong ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bc65f-7e5c-448e-be91-15621806f829",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbbd667-b00a-4f85-9bb4-702a67f5ce53",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively improve the predictions of a weak learner by focusing on the errors made by previous models.\n",
    "Each weak learner is trained to correct the mistakes of its predecessor, gradually reducing the overall error of the ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277711a0-58ff-4fec-b191-46b710323f91",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024cf33-ac30-46e8-ac95-2bf9a7c84154",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble of weak learners sequentially.\n",
    "It starts with an initial model and then iteratively adds new models, each trained to correct the errors of the previous ones.\n",
    "At each iteration, the new model is trained on the residuals (i.e., the differences between the actual and predicted values) of the ensemble's current predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a78970-447c-4c76-8893-beaa3e8b5fd9",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1704ff5-e475-439a-a98c-05335f3ba7fc",
   "metadata": {},
   "source": [
    "Initialize the ensemble with a simple model (often the mean or a constant).\n",
    "Calculate the residuals (errors) between the actual and predicted values.\n",
    "Train a weak learner to predict the residuals.\n",
    "Update the ensemble by adding the predictions of the weak learner, weighted by a learning rate.\n",
    "Repeat steps 2-4 until a predefined number of iterations is reached or until the residuals are minimized to a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0284e75-b162-4485-a7bf-87c4f2fe5c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
