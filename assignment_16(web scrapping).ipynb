{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a595bbfa",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed4f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping is a technique used to extract information from websites. It involves fetching the web page and then extracting \n",
    "# the desired information from the HTML code.\n",
    "# Used:\n",
    "# Data Extraction: Extract specific data from websites lacking convenient APIs.\n",
    "# Competitor Analysis: Gather insights on competitors' prices, products, and reviews.\n",
    "# Research and Analysis: Collect data for market research, sentiment analysis, and trend analysis.\n",
    "    \n",
    "# Areas of Use:\n",
    "# E-Commerce: Track product prices, monitor competitors, and gather product information.\n",
    "# Real Estate: Collect data on property listings, prices, and market trends.\n",
    "# Job Market Analysis: Aggregate job listings, analyze salary information, and skill requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550a8a2",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a10bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Copy-Pasting: Simple, manual extraction of data.\n",
    "# Regular Expression: Uses patterns for data extraction.\n",
    "# HTML Parsing with Libraries: Utilizes tools like BeautifulSoup for structured parsing.\n",
    "# Web Scraping Frameworks: Scrapy and similar tools for complex scraping tasks.\n",
    "# APIs: Access structured data through official interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf1c15",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f25508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup is a Python library for parsing HTML and XML data. It simplifies web scraping .\n",
    "# With robust HTML cleaning capabilities, it facilitates easy extraction of data based on tags and attributes. Compatible with \n",
    "# various parsers, it integrates with other libraries like Requests and Pandas.Its open-source nature and extensive documentation \n",
    "# make it a popular choice for web scraping tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329fbeb",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47448348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask is used in web scraping projects to create a lightweight and flexible web application. \n",
    "# It enables the display of scraped data through a user-friendly interface, enhancing accessibility. \n",
    "# Flask's simplicity allows quick development of web pages to showcase scraped results. \n",
    "# With its integration capabilities, Flask supports dynamic updates as new data is scraped. \n",
    "# Additionally, Flask facilitates easy deployment and hosting, making the scraped information accessible to users. \n",
    "# Overall, Flask enhances the user experience and provides a convenient way to interact with and visualize the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd3a11",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2537c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon EC2 (Elastic Compute Cloud):\n",
    "# Use: EC2 instances can be employed to run web scraping scripts or host the web application. They provide scalable computing \n",
    "# capacity in the cloud.\n",
    "\n",
    "# Amazon S3 (Simple Storage Service):\n",
    "# Use: S3 can store the scraped data, making it accessible and durable. It's a scalable object storage service that allows easy \n",
    "# retrieval and management of data.\n",
    "    \n",
    "# Amazon RDS (Relational Database Service):\n",
    "# Use: RDS can store structured data from the scraping process in a relational database, providing a scalable and managed \n",
    "# database solution.\n",
    "\n",
    "# Amazon API Gateway:\n",
    "# Use: API Gateway can be employed to create APIs for accessing and retrieving the scraped data securely. It acts as a gateway \n",
    "# between the web application and backend services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6b72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
